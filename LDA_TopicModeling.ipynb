{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a algorithms used to discover the topics that are present in a corpus.\n",
    "\n",
    "A few open source libraries exist, but if you are using Python then the main contender is Gensim. Gensim is an awesome library and scales really well to large text corpuses. Gensim, however does not include Non-negative Matrix Factorization (NMF), which can also be used to find topics in text. The mathematical basis underpinning NMF is quite different from LDA. I have found it interesting to compare the results of both of the algorithms and have found that NMF sometimes produces more meaningful topics for smaller datasets. \n",
    "\n",
    "NMF has been included in Scikit Learn for quite a while but LDA has only recently (late 2015) been included. The great thing about using Scikit Learn is that it brings API consistency which makes it almost trivial to perform Topic Modeling using both LDA and NMF. Scikit Learn also includes seeding options for NMF which greatly helps with algorithm convergence and offers both online and batch variants of LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create English stop words list\n",
    "en_stop = stopwords.words('english')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "   \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use sklearn.decomposition.NMF to run top modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.30442406,  0.        ,  0.30442406,  0.        ,\n",
       "        0.        ,  0.        ,  0.20387634,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.24560742,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.30442406,  0.        ,  0.20387634,\n",
       "        0.40775268,  0.        ,  0.        ,  0.30442406,  0.        ,\n",
       "        0.        ,  0.30442406,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.30442406,  0.        ,\n",
       "        0.        ,  0.        ,  0.30442406,  0.20387634,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "tfidf.fit_transform(doc_set).toarray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36398684,  0.36398684,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.24376623,  0.        ,  0.24376623,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.36398684,  0.        ,\n",
       "        0.29366229,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.36398684,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.36398684,  0.        ,  0.        ,\n",
       "        0.36398684,  0.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return [p_stemmer.stem(w) for w in text.split()]\n",
    "\n",
    "tfidf=TfidfVectorizer(stop_words=en_stop, tokenizer=tokenize)\n",
    "tfidf_data=tfidf.fit_transform(doc_set)\n",
    "tfidf_data.toarray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['around',\n",
       " 'basebal',\n",
       " 'better.',\n",
       " 'blood',\n",
       " 'brocolli',\n",
       " 'brocolli,',\n",
       " 'brother',\n",
       " 'caus',\n",
       " 'drive',\n",
       " 'eat',\n",
       " 'eat.',\n",
       " 'expert',\n",
       " 'feel',\n",
       " 'good',\n",
       " 'health',\n",
       " 'health.',\n",
       " 'increas',\n",
       " 'like',\n",
       " 'lot',\n",
       " 'may',\n",
       " 'mother',\n",
       " 'mother.',\n",
       " 'never',\n",
       " 'often',\n",
       " 'perform',\n",
       " 'practice.',\n",
       " 'pressur',\n",
       " 'pressure.',\n",
       " 'profession',\n",
       " 'say',\n",
       " 'school,',\n",
       " 'seem',\n",
       " 'spend',\n",
       " 'suggest',\n",
       " 'tension',\n",
       " 'time',\n",
       " 'well']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta=1, eta=0.1, init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=3, nls_max_iter=2000, random_state=None, shuffle=False,\n",
       "  solver='cd', sparseness=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf=NMF(n_components=3)\n",
    "nmf.fit(tfidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   4.75311216e-01,   2.55432534e-01,\n",
       "          1.62195511e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.55432534e-01,   2.55432534e-01,   0.00000000e+00,\n",
       "          0.00000000e+00,   6.81583674e-01,   2.56015822e-01,\n",
       "          3.33018812e-01,   0.00000000e+00,   2.55432534e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.55432534e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   3.33018812e-01,   3.33018812e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "       [  2.30447562e-01,   2.30447562e-01,   1.95220476e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   5.53711222e-03,\n",
       "          2.88983416e-01,   0.00000000e+00,   2.84307136e-01,\n",
       "          5.53711222e-03,   5.53711222e-03,   0.00000000e+00,\n",
       "          1.95220476e-01,   2.92184283e-03,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   5.53711222e-03,\n",
       "          2.30447562e-01,   0.00000000e+00,   3.43428660e-01,\n",
       "          5.53711222e-03,   1.95220476e-01,   1.95220476e-01,\n",
       "          1.95220476e-01,   2.30447562e-01,   1.95220476e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.95220476e-01,   1.95220476e-01,   2.30447562e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   2.30447562e-01,\n",
       "          1.95220476e-01],\n",
       "       [  9.31838645e-04,   9.31838645e-04,   0.00000000e+00,\n",
       "          3.07861461e-01,   2.81155904e-03,   0.00000000e+00,\n",
       "          0.00000000e+00,   3.07861461e-01,   2.06279844e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   3.07861461e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   2.59817311e-01,\n",
       "          1.35581489e-02,   3.07861461e-01,   0.00000000e+00,\n",
       "          9.31838645e-04,   3.07861461e-01,   1.18469298e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   9.31838645e-04,   0.00000000e+00,\n",
       "          3.07861461e-01,   1.35581489e-02,   1.35581489e-02,\n",
       "          0.00000000e+00,   0.00000000e+00,   9.31838645e-04,\n",
       "          3.07861461e-01,   3.07861461e-01,   9.31838645e-04,\n",
       "          0.00000000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "good brocolli say\n",
      "Topic 1:\n",
      "mother brother drive\n",
      "Topic 2:\n",
      "tension suggest increas\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf, tfidf.get_feature_names(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use sklearn.decomposition.LatentDirichletAllocation to run top modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count=CountVectorizer()\n",
    "ct_data=count.fit_transform(doc_set)\n",
    "ct_data.toarray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_jobs=1, n_topics=3, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda=LatentDirichletAllocation(n_topics=3)\n",
    "lda.fit(ct_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47877873,  1.26048566,  1.27281186,  1.25058509,  1.27062406,\n",
       "         0.47061127,  0.47368621,  2.09892108,  1.28707031,  0.46967821,\n",
       "         1.25348414,  1.27636214,  1.25589913,  0.42678886,  0.43129356,\n",
       "         1.25745904,  0.46417913,  0.48903754,  0.45514668,  0.46225367,\n",
       "         0.4624507 ,  0.4587893 ,  1.28097558,  0.44374677,  2.05333976,\n",
       "         3.68004993,  1.2782428 ,  0.47698271,  1.27108281,  1.27609436,\n",
       "         1.28449295,  1.27622247,  1.24648183,  0.45634715,  0.42442582,\n",
       "         1.28790386,  1.28043624,  0.46580038,  1.26620852,  0.46277578,\n",
       "         0.4667647 ,  0.4504949 ,  1.27565799,  3.69024574,  1.27252745,\n",
       "         0.51128832],\n",
       "       [ 0.46627016,  0.46432638,  0.46692767,  0.46288833,  0.48365718,\n",
       "         0.46076456,  2.07468826,  1.27388109,  1.24424305,  0.49900327,\n",
       "         0.4392863 ,  0.49984064,  0.43360291,  2.1020979 ,  0.43680005,\n",
       "         0.49487926,  0.45019329,  2.06885764,  0.49033502,  0.45825183,\n",
       "         1.2488794 ,  1.27549922,  0.43566624,  0.43734632,  1.28655777,\n",
       "         2.07948149,  0.44436119,  1.29667089,  0.47652188,  0.48708494,\n",
       "         0.47938305,  0.4758734 ,  0.46724404,  0.43411082,  0.46856264,\n",
       "         0.44369107,  0.47793991,  0.50052586,  0.45169869,  0.47245387,\n",
       "         0.48727404,  0.47332297,  0.46616785,  2.06666571,  0.46569942,\n",
       "         0.45000019],\n",
       "       [ 1.26142349,  0.49856128,  0.47173953,  0.457081  ,  0.50307729,\n",
       "         1.26658556,  1.24514068,  0.4475769 ,  0.44319252,  1.24265772,\n",
       "         0.4637518 ,  0.46213702,  1.25359741,  0.46959201,  1.27084193,\n",
       "         0.48018679,  1.31762894,  1.23193234,  2.90030895,  1.22814556,\n",
       "         1.27807086,  0.50133741,  0.4727289 ,  1.28255359,  0.47614661,\n",
       "         0.45052041,  0.48511459,  0.47210817,  0.45961963,  0.46729093,\n",
       "         0.462195  ,  0.4682479 ,  1.27738948,  1.27305941,  1.25413554,\n",
       "         0.47495032,  0.4566817 ,  1.26520361,  0.42018646,  1.28300965,\n",
       "         1.29670491,  2.07161293,  0.45343003,  0.42101887,  0.45931692,\n",
       "         1.25455639]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "to my brother\n",
      "Topic 1:\n",
      "eat my brocolli\n",
      "Topic 2:\n",
      "health that for\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, count.get_feature_names(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use gensim to run topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brocolli',\n",
       "  'good',\n",
       "  'eat',\n",
       "  'brother',\n",
       "  'like',\n",
       "  'eat',\n",
       "  'good',\n",
       "  'brocolli',\n",
       "  'mother'],\n",
       " ['mother',\n",
       "  'spend',\n",
       "  'lot',\n",
       "  'time',\n",
       "  'drive',\n",
       "  'brother',\n",
       "  'around',\n",
       "  'basebal',\n",
       "  'practic'],\n",
       " ['health',\n",
       "  'expert',\n",
       "  'suggest',\n",
       "  'drive',\n",
       "  'may',\n",
       "  'caus',\n",
       "  'increas',\n",
       "  'tension',\n",
       "  'blood',\n",
       "  'pressur'],\n",
       " ['often',\n",
       "  'feel',\n",
       "  'pressur',\n",
       "  'perform',\n",
       "  'well',\n",
       "  'school',\n",
       "  'mother',\n",
       "  'never',\n",
       "  'seem',\n",
       "  'drive',\n",
       "  'brother',\n",
       "  'better'],\n",
       " ['health', 'profession', 'say', 'brocolli', 'good', 'health']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.065*\"drive\" + 0.065*\"health\" + 0.065*\"pressur\"'),\n",
       " (1, '0.082*\"good\" + 0.082*\"brocolli\" + 0.081*\"mother\"'),\n",
       " (2, '0.081*\"health\" + 0.047*\"feel\" + 0.047*\"perform\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "ldamodel.print_topics(num_topics=3, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
